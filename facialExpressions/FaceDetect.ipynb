{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./FacialExpressions_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = '/Users/rt/iCloud Drive (Archive)/Documents/myPersonalProjects/gayan/FacialExpressionDataset/FacialExpressions_800_800/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "df2;\n",
    "for i, row in df.iterrows():\n",
    "    if(i<3000):\n",
    "        image = row['Image ID'] + '.jpg'\n",
    "        output = execCommand('cat \"' + parent + image + '\"| docker run -i think/affectiva')\n",
    "        headers = lines[0].split(',')\n",
    "        if(i==0):\n",
    "            df2 = pd.DataFrame(columns=[headers])\n",
    "            df2['imageId'] = 'i'\n",
    "        lines = output.split('\\n')\n",
    "        values = lines[1].split(',')\n",
    "        values.append(row['Image ID'])\n",
    "#         values = np.array(lines[1].split(','))\n",
    "#         values = values.reshape(1,54)\n",
    "        print(len(values))\n",
    "        print(len(headers))\n",
    "        if(len(values) == len(headers)+1):\n",
    "            df2.loc[i] = values\n",
    "        else:\n",
    "            print(values)\n",
    "#         df2 = pd.concat([df2, values])\n",
    "#         break\n",
    "# print(headers)\n",
    "# print(rows)\n",
    "# df2 = pd.DataFrame(rows, columns=headers)\n",
    "print(df2.head())\n",
    "# print(df2.columns)\n",
    "print(df2['attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execCommand(cmd):\n",
    "    stream = os.popen(cmd)\n",
    "    output = stream.read()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execCommand('pwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execCommand('ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('out2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./out2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['attentiveness'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in df2.iterrows():\n",
    "    for index, row in df.iterrows():\n",
    "        if(row['Image ID'] == r['imageId']):\n",
    "            score = 1\n",
    "            if(row['Attentative Score/0-10'] != 0):\n",
    "                score = row['Attentative Score/0-10']\n",
    "            df2.loc[i, 'attentiveness'] = score\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('out2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TimeStamp=0.0, faceId=0, interocularDistance=245.7521, glasses='no', age='18-24', ethnicity='south asian', gender='male', dominantEmoji='Unknown', pitch=-23.5757, yaw=6.845, roll=-0.1539, joy=0.0016, fear=0.0059, disgust=0.4212, sadness=0.0251, anger=0.004, surprise=0.2828, contempt=0.1928, valence=-4.4333, engagement=3.8688, smile=0.0, innerBrowRaise=3.7619, browRaise=3.8688, browFurrow=0.0004, noseWrinkle=0.0008, upperLipRaise=0.2653, lipCornerDepressor=0.0382, chinRaise=3.9751, lipPucker=3.5008, lipPress=6.1959, lipSuck=0.3115, mouthOpen=0.077, smirk=0.0038, eyeClosure=0.0, attention=95.3955, eyeWiden=0.0455, cheekRaise=0.0, lidTighten=0.0001, dimpler=41.4116, lipStretch=0.0001, jawDrop=59.8029, relaxed=0.0016, smiley=0.0016, laughing=0.0016, kissing=0.0035, disappointed=0.0018, rage=0.004, smirk.1=0.0, wink=0.0018, stuckOutTongueWinkingEye=2.2981, stuckOutTongue=2.2981, flushed=0.0018, scream=0.2483, Unnamed: 53=None, imageId='1User1_10', attentiveness=10.0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "face_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('out2.csv')\n",
    "face_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyspark\n",
    "# house_df.select('gender').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# house_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(house_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "# indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_new\")\n",
    "# house_df = indexer.fit(house_df).transform(house_df)\n",
    "# house_df.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(TimeStamp=0.0, faceId=0, interocularDistance=245.7521, glasses='no', age='18-24', ethnicity='south asian', gender='male', dominantEmoji='Unknown', pitch=-23.5757, yaw=6.845, roll=-0.1539, joy=0.0016, fear=0.0059, disgust=0.4212, sadness=0.0251, anger=0.004, surprise=0.2828, contempt=0.1928, valence=-4.4333, engagement=3.8688, smile=0.0, innerBrowRaise=3.7619, browRaise=3.8688, browFurrow=0.0004, noseWrinkle=0.0008, upperLipRaise=0.2653, lipCornerDepressor=0.0382, chinRaise=3.9751, lipPucker=3.5008, lipPress=6.1959, lipSuck=0.3115, mouthOpen=0.077, smirk=0.0038, eyeClosure=0.0, attention=95.3955, eyeWiden=0.0455, cheekRaise=0.0, lidTighten=0.0001, dimpler=41.4116, lipStretch=0.0001, jawDrop=59.8029, relaxed=0.0016, smiley=0.0016, laughing=0.0016, kissing=0.0035, disappointed=0.0018, rage=0.004, smirk.1=0.0, wink=0.0018, stuckOutTongueWinkingEye=2.2981, stuckOutTongue=2.2981, flushed=0.0018, scream=0.2483, Unnamed: 53=None, imageId='1User1_10', attentiveness=10.0, features=DenseVector([0.0016, 0.0059, 0.4212, 0.0251, 0.004, 0.2828, 0.1928, -4.4333, 3.8688, 0.0, 3.7619, 3.8688, 0.0004, 0.0008, 0.2653, 0.0382, 3.9751, 3.5008, 6.1959, 0.3115, 0.077, 0.0038, 0.0, 0.0455, 0.0, 0.0001, 41.4116, 0.0001, 59.8029, 0.0016, 0.0016, 0.0016, 0.0035, 0.0018]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['joy', 'fear', 'disgust', 'sadness', 'anger', 'surprise', 'contempt', 'valence', 'engagement', 'smile', 'innerBrowRaise', 'browRaise', 'browFurrow', 'noseWrinkle', 'upperLipRaise', 'lipCornerDepressor', 'chinRaise', 'lipPucker', 'lipPress', 'lipSuck', 'mouthOpen', 'smirk', 'eyeClosure', 'eyeWiden', 'cheekRaise', 'lidTighten', 'dimpler', 'lipStretch', 'jawDrop', 'relaxed', 'smiley', 'laughing', 'kissing', 'disappointed'], outputCol = 'features')\n",
    "vface_df = vectorAssembler.transform(face_df)\n",
    "vface_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|attentiveness|\n",
      "+--------------------+-------------+\n",
      "|[0.0016,0.0059,0....|         10.0|\n",
      "|[0.0013,0.006,0.4...|          5.0|\n",
      "|[0.0018,0.0052,0....|          9.0|\n",
      "+--------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normalizer = Normalizer(inputCol=\"features\", outputCol=\"scaledFeatures\", p=1.0)\n",
    "# vhouse_df = normalizer.transform(vhouse_df)\n",
    "\n",
    "# scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "# scalerModel = scaler.fit(vhouse_df)\n",
    "# vhouse_df = scalerModel.transform(vhouse_df)\n",
    "\n",
    "\n",
    "# vhouse_df = vhouse_df.select(['scaledFeatures', 'attentiveness'])\n",
    "vface_df = vface_df.select(['features', 'attentiveness'])\n",
    "vface_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vface_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.003885541497127465,0.0,0.0,-0.008099264668841916,0.0,-0.009035215939855653,0.0,0.0,-3.994049905867132e-05,0.0,0.0,0.0,0.0,0.0,-0.044613367254392515,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.010299957759388445]\n",
      "Intercept: 6.147538542785145\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='attentiveness', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.084876\n",
      "r2: 0.152488\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|     attentiveness|\n",
      "+-------+------------------+\n",
      "|  count|               739|\n",
      "|   mean|5.7347767253044655|\n",
      "| stddev| 3.353196153217222|\n",
      "|    min|               1.0|\n",
      "|    max|              10.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+----------+\n",
      "|       prediction|attentiveness|  features|\n",
      "+-----------------+-------------+----------+\n",
      "|6.117771779183713|          1.0|(34,[],[])|\n",
      "|6.117771779183713|          1.0|(34,[],[])|\n",
      "|6.117771779183713|          1.0|(34,[],[])|\n",
      "|6.117771779183713|          1.0|(34,[],[])|\n",
      "|6.117771779183713|          6.0|(34,[],[])|\n",
      "+-----------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.160105\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"attentiveness\",\"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"attentiveness\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.05679\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+--------------------+\n",
      "|        prediction|attentiveness|            features|\n",
      "+------------------+-------------+--------------------+\n",
      "| 6.117771779183713|          1.0|          (34,[],[])|\n",
      "| 6.117771779183713|          1.0|          (34,[],[])|\n",
      "| 6.117771779183713|          1.0|          (34,[],[])|\n",
      "| 6.117771779183713|          1.0|          (34,[],[])|\n",
      "| 6.117771779183713|          6.0|          (34,[],[])|\n",
      "| 6.117771779183713|          7.0|          (34,[],[])|\n",
      "| 6.117387914916791|          7.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.070065355883991|          4.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.115278901635902|          7.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.117343458409349|          8.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.117332529221051|          7.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.117303445597998|          8.0|(34,[0,1,2,3,4,5,...|\n",
      "| 5.593763176989088|          1.0|(34,[0,1,2,3,4,5,...|\n",
      "|6.1173893267540675|          7.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.117387374909611|          4.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.116377295403572|          7.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.117391245812078|          1.0|(34,[0,1,2,3,4,5,...|\n",
      "| 6.117391049093406|          7.0|(34,[0,1,2,3,4,5,...|\n",
      "|  2.28291803495192|          1.0|(34,[0,1,2,3,4,5,...|\n",
      "|6.1173870031782025|          9.0|(34,[0,1,2,3,4,5,...|\n",
      "+------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"attentiveness\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LinearRegression_b0fafdd856ba', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LinearRegression_b0fafdd856ba', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.8, Param(parent='LinearRegression_b0fafdd856ba', name='featuresCol', doc='features column name.'): 'features', Param(parent='LinearRegression_b0fafdd856ba', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LinearRegression_b0fafdd856ba', name='labelCol', doc='label column name.'): 'attentiveness', Param(parent='LinearRegression_b0fafdd856ba', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LinearRegression_b0fafdd856ba', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto', Param(parent='LinearRegression_b0fafdd856ba', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LinearRegression_b0fafdd856ba', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35, Param(parent='LinearRegression_b0fafdd856ba', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError', Param(parent='LinearRegression_b0fafdd856ba', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LinearRegression_b0fafdd856ba', name='regParam', doc='regularization parameter (>= 0).'): 0.3, Param(parent='LinearRegression_b0fafdd856ba', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "## Decision tree regression\n",
    "print(lr_model.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.84683\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`probability`' given input columns: [attentiveness, features, prediction];;\n'Project [prediction#648, attentiveness#71, 'probability, features#184]\n+- Project [features#184, attentiveness#71, UDF(features#184) AS prediction#648]\n   +- Sample 0.7, 1.0, false, 2166749806685108746\n      +- Sort [features#184 ASC NULLS FIRST, attentiveness#71 ASC NULLS FIRST], false\n         +- Project [features#184, attentiveness#71]\n            +- Project [TimeStamp#16, faceId#17, interocularDistance#18, glasses#19, age#20, ethnicity#21, gender#22, dominantEmoji#23, pitch#24, yaw#25, roll#26, joy#27, fear#28, disgust#29, sadness#30, anger#31, surprise#32, contempt#33, valence#34, engagement#35, smile#36, innerBrowRaise#37, browRaise#38, browFurrow#39, ... 33 more fields]\n               +- Relation[TimeStamp#16,faceId#17,interocularDistance#18,glasses#19,age#20,ethnicity#21,gender#22,dominantEmoji#23,pitch#24,yaw#25,roll#26,joy#27,fear#28,disgust#29,sadness#30,anger#31,surprise#32,contempt#33,valence#34,engagement#35,smile#36,innerBrowRaise#37,browRaise#38,browFurrow#39,... 32 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fea2ab965a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root Mean Squared Error (RMSE) on test data = %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdt_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attentiveness'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`probability`' given input columns: [attentiveness, features, prediction];;\n'Project [prediction#648, attentiveness#71, 'probability, features#184]\n+- Project [features#184, attentiveness#71, UDF(features#184) AS prediction#648]\n   +- Sample 0.7, 1.0, false, 2166749806685108746\n      +- Sort [features#184 ASC NULLS FIRST, attentiveness#71 ASC NULLS FIRST], false\n         +- Project [features#184, attentiveness#71]\n            +- Project [TimeStamp#16, faceId#17, interocularDistance#18, glasses#19, age#20, ethnicity#21, gender#22, dominantEmoji#23, pitch#24, yaw#25, roll#26, joy#27, fear#28, disgust#29, sadness#30, anger#31, surprise#32, contempt#33, valence#34, engagement#35, smile#36, innerBrowRaise#37, browRaise#38, browFurrow#39, ... 33 more fields]\n               +- Relation[TimeStamp#16,faceId#17,interocularDistance#18,glasses#19,age#20,ethnicity#21,gender#22,dominantEmoji#23,pitch#24,yaw#25,roll#26,joy#27,fear#28,disgust#29,sadness#30,anger#31,surprise#32,contempt#33,valence#34,engagement#35,smile#36,innerBrowRaise#37,browRaise#38,browFurrow#39,... 32 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'attentiveness')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"attentiveness\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "dt_predictions.select('prediction', 'attentiveness','probability', 'features').show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+----------+\n",
      "|        prediction|attentiveness|  features|\n",
      "+------------------+-------------+----------+\n",
      "|4.1745111723772865|          1.0|(34,[],[])|\n",
      "|4.1745111723772865|          1.0|(34,[],[])|\n",
      "|4.1745111723772865|          1.0|(34,[],[])|\n",
      "|4.1745111723772865|          1.0|(34,[],[])|\n",
      "|4.1745111723772865|          1.0|(34,[],[])|\n",
      "+------------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'attentiveness', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'attentiveness', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.76647\n"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"attentiveness\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.2.0.34-cp37-cp37m-macosx_10_9_x86_64.whl (49.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 49.1 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/site-packages (from opencv-python) (1.18.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.34\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "df2;\n",
    "for i, row in df.iterrows():\n",
    "    if(i<3000):\n",
    "        image = row['Image ID'] + '.jpg'\n",
    "        output = execCommand('cat \"' + parent + image + '\"| docker run -i think/affectiva')\n",
    "        headers = lines[0].split(',')\n",
    "        if(i==0):\n",
    "            df2 = pd.DataFrame(columns=[headers])\n",
    "            df2['imageId'] = 'i'\n",
    "        lines = output.split('\\n')\n",
    "        values = lines[1].split(',')\n",
    "        values.append(row['Image ID'])\n",
    "#         values = np.array(lines[1].split(','))\n",
    "#         values = values.reshape(1,54)\n",
    "        print(len(values))\n",
    "        print(len(headers))\n",
    "        if(len(values) == len(headers)+1):\n",
    "            df2.loc[i] = values\n",
    "        else:\n",
    "            print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
